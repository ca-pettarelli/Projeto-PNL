{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(sentence):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            phrase.append(word)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167552\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/11/11-0.txt\" \n",
    "livro = request.urlopen(url)\n",
    "livro_raw = livro.read().decode('utf-8-sig') \n",
    "print(len(livro_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: Alice’s Adventures in Wonderland\r\n",
      "\r\n",
      "Author\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(livro_raw[250:380])\n",
    "print(type(livro_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s Alice’s Ad\n",
      "['Project', 'Gutenberg', '’', 's', 'Alice', '’', 's', 'Adventures', 'in', 'Wonderland', ',', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no']\n",
      "['de', '!', 'i', 'shall', 'be', 'lat', '!', '’', '(', 'when', 'she', 'thought', 'it', 'ov', 'afterward', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'hav', 'wondered', 'at', 'thil', ',', 'but']\n",
      "['ov', 'afterward', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'hav', 'wondered', 'at', 'thil', ',', 'but', 'at', 'the', 'tim', 'it', 'all', 'seemed', 'quit', 'natur', ')', ';', 'but', 'when', 'the']\n",
      "['project', 'gutenberg', 's', 'alic', 's', 'adventur', 'in', 'wonderland', 'by', 'lewil', 'carroll', 'thil', 'ebook', 'is', 'the', 'use', 'of', 'anyon', 'anywh', 'at', 'cost', 'and', 'with', 'almost', 'restrictiom', 'whatsoev', 'you', 'may', 'copy', 'it']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# sentences = nltk.sent_tokenize(livro_raw)\n",
    "\n",
    "\n",
    "print(livro_raw[:30])\n",
    "\n",
    "sentences = nltk.sent_tokenize(livro_raw)\n",
    "\n",
    "livro = []\n",
    "for i in sentences:\n",
    "    livro.append(Tokenize(i))\n",
    "\n",
    "words =[]\n",
    "for i in sentences:\n",
    "    words.append(word_tokenize(i))\n",
    "    \n",
    "# tokens = word_tokenize(livro_raw)\n",
    "# print(tokens[:30])\n",
    "# tl=[]\n",
    "# for t in tokens:\n",
    "#   tl.append(t)\n",
    "\n",
    "livro = []\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "for w in tokens:\n",
    "    livro.append(stemmer.stem(w))\n",
    "    \n",
    "print(livro[300:330])\n",
    "\n",
    "tl2=[]\n",
    "for t in livro:\n",
    "    if t not in stopwords:\n",
    "        tl2.append(t)\n",
    "#sentences = nltk.sent_tokenize(livro_raw)\n",
    "\n",
    "# livro = []\n",
    "# for i in sentences:\n",
    "#     livro.append(Tokenize(i))\n",
    "    \n",
    "#livro = Tokenize(livro)\n",
    "\n",
    "# livro = Stemming(words)\n",
    "# print(livro[215:230])\n",
    "\n",
    "# livro = RemoveStopWords(livro)\n",
    "# print(livro[215:230])\n",
    "\n",
    "# words = [word for word in livro if word.isalpha()]\n",
    "\n",
    "# print(words[215:230])\n",
    "\n",
    "# print(type(words))\n",
    "\n",
    "   \n",
    "print(tl2[300:330])\n",
    "words = [word for word in tl2 if word.isalpha()]\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = nltk.stem.RSLPStemmer()\n",
    "# livro = []\n",
    "# for w in words:\n",
    "#     livro.append(stemmer.stem(w))\n",
    "# print(livro[300:330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# tl=[]\n",
    "# for t in livro:\n",
    "#     if t not in stopwords:\n",
    "#         tl.append(t.lower())\n",
    "        \n",
    "# print(tl[300:330])\n",
    "# words = [word for word in tl if word.isalpha()]\n",
    "# print(words[300:330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=26, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(words)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'o', 'j', 'e', 'c', 't', 'g', 'u', 'n', 'b', 's', 'a', 'l', 'i', 'd', 'v', 'w', 'y', 'h', 'k', 'f', 'm', 'q', 'x', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(model.wv.vocab)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
